{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a671c9681314b19aa78e0f0547897b3": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f9a105089b064124b4c88e78a540daa6",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Statistics collection \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[38;2;0;104;181m300/300\u001b[0m • \u001b[38;2;0;104;181m0:00:17\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Statistics collection <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #0068b5; text-decoration-color: #0068b5\">300/300</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:17</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f9a105089b064124b4c88e78a540daa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac1c5bfbd4e49fdb76b4736b871fdf1": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ab063f91377c4d7e983aff42e94aa0e1",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Applying Fast Bias correction \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[38;2;0;104;181m16/16\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Applying Fast Bias correction <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #0068b5; text-decoration-color: #0068b5\">16/16</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:00</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "ab063f91377c4d7e983aff42e94aa0e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19dd26c59eda4892b0248c4e214b274b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "CPU",
              "AUTO"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Device:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_2b8950572ea54f99a5fd60ad675f0b65",
            "style": "IPY_MODEL_4b34519b255c475682c659e5edab7f24"
          }
        },
        "2b8950572ea54f99a5fd60ad675f0b65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b34519b255c475682c659e5edab7f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 변환 및 경량화, 평가"
      ],
      "metadata": {
        "id": "KbMbI6h6Y-fK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NywvKOULY8tL"
      },
      "outputs": [],
      "source": [
        "!unzip -qq \"saved_model.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q \"openvino>=2023.1.0\" \"nncf>=2.6.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0-j4tOccpji",
        "outputId": "7f508f66-ad0f-451f-c2fc-8c4b9b38fe60"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.5/37.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## saved_model모델을 openvino로 변환"
      ],
      "metadata": {
        "id": "UP3SC00AZGHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openvino as ov\n",
        "\n",
        "ov_model = ov.convert_model('saved_model', input=[1,224,224,3])\n",
        "ov.save_model(ov_model, \"talkFinger_edit.xml\")"
      ],
      "metadata": {
        "id": "lozjhyjuctNA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 변환된 모델의 추론시간"
      ],
      "metadata": {
        "id": "mV2o5d0JewvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openvino.runtime import Core\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "model_path = \"talkFinger_edit.xml\"\n",
        "ie = Core() # initialize inference engine\n",
        "network = ie.read_model(model=model_path, weights=Path(model_path).with_suffix('.bin'))\n",
        "executable_network = ie.compile_model(model=network, device_name=\"CPU\")\n",
        "\n",
        "im = np.random.randn(1, 224, 224 , 3) #random input\n",
        "output_layer = next(iter(executable_network.outputs)) # OpenVINO model의 output layer를 가져옴\n",
        "y = executable_network([im])[output_layer] # Inference 실행하여 output_layer에 해당하는 output을 y에 할당\n",
        "\n",
        "start = time.time()\n",
        "y = executable_network([im]) [output_layer]\n",
        "print(f'time lapse: {time.time()-start}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga4DwKWRerhv",
        "outputId": "fb83fb53-13b7-4f91-de9c-3f4219a63b97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time lapse: 0.02854776382446289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 불러오는 함수"
      ],
      "metadata": {
        "id": "RdjZ06HzdEvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnQMWlpvcyVG",
        "outputId": "ae74fdd7-aebf-47eb-f79f-8e919209c90b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def load_data(directory):\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        img = Image.open(os.path.join(directory, filename))\n",
        "        img = transform(img)\n",
        "        images.append(img)\n",
        "    return images"
      ],
      "metadata": {
        "id": "VgCPPGhLcz7z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 프레임 단위로 캡처해서 저장하기"
      ],
      "metadata": {
        "id": "By5WmGDFdNwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "video_path1 = 'KETI_SL_0000008142.avi'\n",
        "video_path2 = 'KETI_SL_0000008143.avi'\n",
        "video_path3 = 'KETI_SL_0000008144.avi'\n",
        "video_path4 = 'KETI_SL_0000008145.avi'\n",
        "\n",
        "save_dir = 'video_capture_datatset'\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "cap1 = cv2.VideoCapture(video_path1)\n",
        "cap2 = cv2.VideoCapture(video_path2)\n",
        "cap3 = cv2.VideoCapture(video_path3)\n",
        "cap4 = cv2.VideoCapture(video_path4)\n",
        "\n",
        "frame_count1 = 0\n",
        "frame_count2 = 76\n",
        "frame_count3 = 158\n",
        "frame_count4 = 258\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap1.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # 프레임 이미지 파일로 저장\n",
        "    save_path = os.path.join(save_dir, f'frame_{frame_count1}.jpg')\n",
        "    cv2.imwrite(save_path, frame)\n",
        "    frame_count1 += 1\n",
        "\n",
        "cap1.release()\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap2.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # 프레임 이미지 파일로 저장\n",
        "    save_path = os.path.join(save_dir, f'frame_{frame_count2}.jpg')\n",
        "    cv2.imwrite(save_path, frame)\n",
        "    frame_count2 += 1\n",
        "\n",
        "cap2.release()\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap3.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # 프레임 이미지 파일로 저장\n",
        "    save_path = os.path.join(save_dir, f'frame_{frame_count3}.jpg')\n",
        "    cv2.imwrite(save_path, frame)\n",
        "    frame_count3 += 1\n",
        "\n",
        "cap3.release()\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap4.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # 프레임 이미지 파일로 저장\n",
        "    save_path = os.path.join(save_dir, f'frame_{frame_count4}.jpg')\n",
        "    cv2.imwrite(save_path, frame)\n",
        "    frame_count4 += 1\n",
        "\n",
        "cap4.release()"
      ],
      "metadata": {
        "id": "5BxlPe9vc2Rz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리 및 정규화"
      ],
      "metadata": {
        "id": "FIF-HafmeDBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "dataset = load_data(\"video_capture_datatset\")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        ")"
      ],
      "metadata": {
        "id": "zeAa4sLmdDLB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 직접 구현한 최적화 로직"
      ],
      "metadata": {
        "id": "1if1-TlyeJgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nncf\n",
        "\n",
        "def transform_fn(data_item):\n",
        "    image_tensor = data_item[0]\n",
        "    # HWC 형태를 CHW 형태로 변환\n",
        "    image_tensor = image_tensor.permute(2, 0, 1)\n",
        "    # 배치 차원 추가\n",
        "    image_tensor = image_tensor.unsqueeze(0)\n",
        "    # 채널 차원을 올바른 위치로 이동 (현재: BCHW, 목표: BHWC)\n",
        "    image_tensor = image_tensor.permute(0, 1, 3, 2)\n",
        "    return image_tensor.numpy()\n",
        "\n",
        "quantization_dataset = nncf.Dataset(val_loader, transform_fn)\n",
        "\n",
        "quant_ov_model = nncf.quantize(ov_model, quantization_dataset)\n",
        "ov.save_model(quant_ov_model, \"quantized_talkFinger.xml\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "6a671c9681314b19aa78e0f0547897b3",
            "f9a105089b064124b4c88e78a540daa6",
            "9ac1c5bfbd4e49fdb76b4736b871fdf1",
            "ab063f91377c4d7e983aff42e94aa0e1"
          ]
        },
        "id": "ttxBFznMeHgT",
        "outputId": "9512b315-c80b-41bd-fb65-c8a8641b9fe5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, openvino\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a671c9681314b19aa78e0f0547897b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ac1c5bfbd4e49fdb76b4736b871fdf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 성능 평가"
      ],
      "metadata": {
        "id": "oepKkJS1eO9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "import openvino as ov"
      ],
      "metadata": {
        "id": "0KwGQEUfeTmR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "core = ov.Core()\n",
        "device = widgets.Dropdown(\n",
        "    options=core.available_devices + [\"AUTO\"],\n",
        "    value='AUTO',\n",
        "    description='Device:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "19dd26c59eda4892b0248c4e214b274b",
            "2b8950572ea54f99a5fd60ad675f0b65",
            "4b34519b255c475682c659e5edab7f24"
          ]
        },
        "id": "wWu9hPsnea7B",
        "outputId": "abf466e3-b6ee-45b9-bde7-a8f28c002467"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19dd26c59eda4892b0248c4e214b274b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference FP32 model (OpenVINO IR)\n",
        "!benchmark_app -m \"talkFinger_edit.xml\" -d $device.value -api async -t 15 -shape \"conv2d_input:[1,3,224,224]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXiXi1VFedFj",
        "outputId": "9305611c-7fd3-46b4-e1f1-e301d0a429f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 1/11] Parsing and validating input arguments\n",
            "[ INFO ] Parsing input parameters\n",
            "[Step 2/11] Loading OpenVINO Runtime\n",
            "[ INFO ] OpenVINO:\n",
            "[ INFO ] Build ................................. 2023.2.0-13089-cfd42bd2cb0-HEAD\n",
            "[ INFO ] \n",
            "[ INFO ] Device info:\n",
            "[ INFO ] AUTO\n",
            "[ INFO ] Build ................................. 2023.2.0-13089-cfd42bd2cb0-HEAD\n",
            "[ INFO ] \n",
            "[ INFO ] \n",
            "[Step 3/11] Setting device configuration\n",
            "[ WARNING ] Performance hint was not explicitly specified in command line. Device(AUTO) performance hint will be set to PerformanceMode.THROUGHPUT.\n",
            "[Step 4/11] Reading model files\n",
            "[ INFO ] Loading model files\n",
            "[ INFO ] Read model took 10.46 ms\n",
            "[ INFO ] Original model I/O parameters:\n",
            "[ INFO ] Model inputs:\n",
            "[ INFO ]     input_2 (node: input_2) : f32 / [...] / [1,224,224,3]\n",
            "[ INFO ] Model outputs:\n",
            "[ INFO ]     dense_3 (node: model_1/dense_3/Softmax) : f32 / [...] / [1,50]\n",
            "[Step 5/11] Resizing model to match image sizes and given batch\n",
            "[ INFO ] Model batch size: 1\n",
            "[Step 6/11] Configuring input of the model\n",
            "[ INFO ] Model inputs:\n",
            "[ INFO ]     input_2 (node: input_2) : u8 / [N,H,W,C] / [1,224,224,3]\n",
            "[ INFO ] Model outputs:\n",
            "[ INFO ]     dense_3 (node: model_1/dense_3/Softmax) : f32 / [...] / [1,50]\n",
            "[Step 7/11] Loading the model to the device\n",
            "[ INFO ] Compile model took 129.29 ms\n",
            "[Step 8/11] Querying optimal runtime parameters\n",
            "[ INFO ] Model:\n",
            "[ INFO ]   NETWORK_NAME: TensorFlow_Frontend_IR\n",
            "[ INFO ]   EXECUTION_DEVICES: ['CPU']\n",
            "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
            "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 2\n",
            "[ INFO ]   MULTI_DEVICE_PRIORITIES: CPU\n",
            "[ INFO ]   CPU:\n",
            "[ INFO ]     AFFINITY: Affinity.CORE\n",
            "[ INFO ]     CPU_DENORMALS_OPTIMIZATION: False\n",
            "[ INFO ]     CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1.0\n",
            "[ INFO ]     ENABLE_CPU_PINNING: True\n",
            "[ INFO ]     ENABLE_HYPER_THREADING: True\n",
            "[ INFO ]     EXECUTION_DEVICES: ['CPU']\n",
            "[ INFO ]     EXECUTION_MODE_HINT: ExecutionMode.PERFORMANCE\n",
            "[ INFO ]     INFERENCE_NUM_THREADS: 2\n",
            "[ INFO ]     INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
            "[ INFO ]     NETWORK_NAME: TensorFlow_Frontend_IR\n",
            "[ INFO ]     NUM_STREAMS: 2\n",
            "[ INFO ]     OPTIMAL_NUMBER_OF_INFER_REQUESTS: 2\n",
            "[ INFO ]     PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
            "[ INFO ]     PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
            "[ INFO ]     PERF_COUNT: False\n",
            "[ INFO ]     SCHEDULING_CORE_TYPE: SchedulingCoreType.ANY_CORE\n",
            "[ INFO ]   MODEL_PRIORITY: Priority.MEDIUM\n",
            "[ INFO ]   LOADED_FROM_CACHE: False\n",
            "[Step 9/11] Creating infer requests and preparing input tensors\n",
            "[ WARNING ] No input files were given for input 'input_2'!. This input will be filled with random values!\n",
            "[ INFO ] Fill input 'input_2' with random values \n",
            "[Step 10/11] Measuring performance (Start inference asynchronously, 2 inference requests, limits: 15000 ms duration)\n",
            "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
            "[ INFO ] First inference took 22.22 ms\n",
            "[Step 11/11] Dumping statistics report\n",
            "[ INFO ] Execution Devices:['CPU']\n",
            "[ INFO ] Count:            726 iterations\n",
            "[ INFO ] Duration:         15032.95 ms\n",
            "[ INFO ] Latency:\n",
            "[ INFO ]    Median:        31.56 ms\n",
            "[ INFO ]    Average:       41.26 ms\n",
            "[ INFO ]    Min:           25.90 ms\n",
            "[ INFO ]    Max:           214.90 ms\n",
            "[ INFO ] Throughput:   48.29 FPS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference INT8 model (OpenVINO IR)\n",
        "!benchmark_app -m \"quantized_talkFinger.xml\" -d $device.value -api async -t 15 -shape \"conv2d_input:[32,224,224,3]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwhRtfG4ei-Y",
        "outputId": "c04aa0c2-2b5b-49ce-b51c-e17f1b60c0e3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 1/11] Parsing and validating input arguments\n",
            "[ INFO ] Parsing input parameters\n",
            "[Step 2/11] Loading OpenVINO Runtime\n",
            "[ INFO ] OpenVINO:\n",
            "[ INFO ] Build ................................. 2023.2.0-13089-cfd42bd2cb0-HEAD\n",
            "[ INFO ] \n",
            "[ INFO ] Device info:\n",
            "[ INFO ] AUTO\n",
            "[ INFO ] Build ................................. 2023.2.0-13089-cfd42bd2cb0-HEAD\n",
            "[ INFO ] \n",
            "[ INFO ] \n",
            "[Step 3/11] Setting device configuration\n",
            "[ WARNING ] Performance hint was not explicitly specified in command line. Device(AUTO) performance hint will be set to PerformanceMode.THROUGHPUT.\n",
            "[Step 4/11] Reading model files\n",
            "[ INFO ] Loading model files\n",
            "[ INFO ] Read model took 20.50 ms\n",
            "[ INFO ] Original model I/O parameters:\n",
            "[ INFO ] Model inputs:\n",
            "[ INFO ]     input_2 (node: input_2) : f32 / [...] / [1,224,224,3]\n",
            "[ INFO ] Model outputs:\n",
            "[ INFO ]     dense_3 (node: model_1/dense_3/Softmax) : f32 / [...] / [1,50]\n",
            "[Step 5/11] Resizing model to match image sizes and given batch\n",
            "[ INFO ] Model batch size: 1\n",
            "[Step 6/11] Configuring input of the model\n",
            "[ INFO ] Model inputs:\n",
            "[ INFO ]     input_2 (node: input_2) : u8 / [N,H,W,C] / [1,224,224,3]\n",
            "[ INFO ] Model outputs:\n",
            "[ INFO ]     dense_3 (node: model_1/dense_3/Softmax) : f32 / [...] / [1,50]\n",
            "[Step 7/11] Loading the model to the device\n",
            "[ INFO ] Compile model took 217.99 ms\n",
            "[Step 8/11] Querying optimal runtime parameters\n",
            "[ INFO ] Model:\n",
            "[ INFO ]   NETWORK_NAME: TensorFlow_Frontend_IR\n",
            "[ INFO ]   EXECUTION_DEVICES: ['CPU']\n",
            "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
            "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 2\n",
            "[ INFO ]   MULTI_DEVICE_PRIORITIES: CPU\n",
            "[ INFO ]   CPU:\n",
            "[ INFO ]     AFFINITY: Affinity.CORE\n",
            "[ INFO ]     CPU_DENORMALS_OPTIMIZATION: False\n",
            "[ INFO ]     CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1.0\n",
            "[ INFO ]     ENABLE_CPU_PINNING: True\n",
            "[ INFO ]     ENABLE_HYPER_THREADING: True\n",
            "[ INFO ]     EXECUTION_DEVICES: ['CPU']\n",
            "[ INFO ]     EXECUTION_MODE_HINT: ExecutionMode.PERFORMANCE\n",
            "[ INFO ]     INFERENCE_NUM_THREADS: 2\n",
            "[ INFO ]     INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
            "[ INFO ]     NETWORK_NAME: TensorFlow_Frontend_IR\n",
            "[ INFO ]     NUM_STREAMS: 2\n",
            "[ INFO ]     OPTIMAL_NUMBER_OF_INFER_REQUESTS: 2\n",
            "[ INFO ]     PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
            "[ INFO ]     PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
            "[ INFO ]     PERF_COUNT: False\n",
            "[ INFO ]     SCHEDULING_CORE_TYPE: SchedulingCoreType.ANY_CORE\n",
            "[ INFO ]   MODEL_PRIORITY: Priority.MEDIUM\n",
            "[ INFO ]   LOADED_FROM_CACHE: False\n",
            "[Step 9/11] Creating infer requests and preparing input tensors\n",
            "[ WARNING ] No input files were given for input 'input_2'!. This input will be filled with random values!\n",
            "[ INFO ] Fill input 'input_2' with random values \n",
            "[Step 10/11] Measuring performance (Start inference asynchronously, 2 inference requests, limits: 15000 ms duration)\n",
            "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
            "[ INFO ] First inference took 15.78 ms\n",
            "[Step 11/11] Dumping statistics report\n",
            "[ INFO ] Execution Devices:['CPU']\n",
            "[ INFO ] Count:            1186 iterations\n",
            "[ INFO ] Duration:         15027.26 ms\n",
            "[ INFO ] Latency:\n",
            "[ INFO ]    Median:        21.55 ms\n",
            "[ INFO ]    Average:       25.19 ms\n",
            "[ INFO ]    Min:           13.29 ms\n",
            "[ INFO ]    Max:           75.62 ms\n",
            "[ INFO ] Throughput:   78.92 FPS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intel에서 제공해준 예시코드 변형한 최적화 로직"
      ],
      "metadata": {
        "id": "9VHu8eHMe_5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2023 Intel Corporation\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "\n",
        "import openvino as ov\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nncf\n",
        "\n",
        "ROOT = os.getcwd()\n",
        "WEIGHTS_URL = \"https://huggingface.co/alexsu52/mobilenet_v2_imagenette/resolve/main/tf_model.h5\"\n",
        "DATASET_CLASSES = 10\n",
        "\n",
        "\n",
        "def validate(model: ov.Model, val_loader: tf.data.Dataset) -> tf.Tensor:\n",
        "    compiled_model = ov.compile_model(model)\n",
        "    output = compiled_model.outputs[0]\n",
        "\n",
        "    metric = tf.keras.metrics.CategoricalAccuracy(name=\"acc@1\")\n",
        "    for images, labels in tqdm(val_loader):\n",
        "        pred = compiled_model(images.numpy())[output]\n",
        "        metric.update_state(labels, pred)\n",
        "\n",
        "    return metric.result()\n",
        "\n",
        "\n",
        "def run_benchmark(model_path: str, shape: Optional[List[int]] = None, verbose: bool = True) -> float:\n",
        "    command = f\"benchmark_app -m {model_path} -d CPU -api async -t 15\"\n",
        "    if shape is not None:\n",
        "        command += f' -shape [{\",\".join(str(x) for x in shape)}]'\n",
        "    cmd_output = subprocess.check_output(command, shell=True)  # nosec\n",
        "    if verbose:\n",
        "        print(*str(cmd_output).split(\"\\\\n\")[-9:-1], sep=\"\\n\")\n",
        "    match = re.search(r\"Throughput\\: (.+?) FPS\", str(cmd_output))\n",
        "    return float(match.group(1))\n",
        "\n",
        "\n",
        "def get_model_size(ir_path: str, m_type: str = \"Mb\", verbose: bool = True) -> float:\n",
        "    xml_size = os.path.getsize(ir_path)\n",
        "    bin_size = os.path.getsize(os.path.splitext(ir_path)[0] + \".bin\")\n",
        "    for t in [\"bytes\", \"Kb\", \"Mb\"]:\n",
        "        if m_type == t:\n",
        "            break\n",
        "        xml_size /= 1024\n",
        "        bin_size /= 1024\n",
        "    model_size = xml_size + bin_size\n",
        "    if verbose:\n",
        "        print(f\"Model graph (xml):   {xml_size:.3f} Mb\")\n",
        "        print(f\"Model weights (bin): {bin_size:.3f} Mb\")\n",
        "        print(f\"Model size:          {model_size:.3f} Mb\")\n",
        "    return model_size\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Create a Tensorflow model and dataset\n",
        "\n",
        "\n",
        "def center_crop(image: tf.Tensor, image_size: int, crop_padding: int) -> tf.Tensor:\n",
        "    shape = tf.shape(image)\n",
        "    image_height = shape[0]\n",
        "    image_width = shape[1]\n",
        "\n",
        "    padded_center_crop_size = tf.cast(\n",
        "        ((image_size / (image_size + crop_padding)) * tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n",
        "        tf.int32,\n",
        "    )\n",
        "\n",
        "    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n",
        "    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n",
        "\n",
        "    image = tf.image.crop_to_bounding_box(\n",
        "        image,\n",
        "        offset_height=offset_height,\n",
        "        offset_width=offset_width,\n",
        "        target_height=padded_center_crop_size,\n",
        "        target_width=padded_center_crop_size,\n",
        "    )\n",
        "\n",
        "    image = tf.compat.v1.image.resize(\n",
        "        image, [image_size, image_size], method=tf.image.ResizeMethod.BILINEAR, align_corners=False\n",
        "    )\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def preprocess_for_eval(image, label):\n",
        "    image = center_crop(image, 224, 32)\n",
        "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "    label = tf.one_hot(label, DATASET_CLASSES)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "val_dataset = tfds.load(\"imagenette/320px-v2\", split=\"validation\", shuffle_files=False, as_supervised=True)\n",
        "val_dataset = val_dataset.map(preprocess_for_eval).batch(128)\n",
        "\n",
        "# 모델 경로 업데이트\n",
        "weights_path = r\"talkfinger.h5\"\n",
        "\n",
        "# Keras 모델 로딩 방식을 파일 경로를 사용하는 방식으로 변경\n",
        "tf_model = tf.keras.models.load_model(weights_path)\n",
        "\n",
        "###############################################################################\n",
        "# Quantize a Tensorflow model\n",
        "#\n",
        "# The transformation function transforms a data item into model input data.\n",
        "#\n",
        "# To validate the transform function use the following code:\n",
        "# >> for data_item in val_loader:\n",
        "# >>    model(transform_fn(data_item))\n",
        "\n",
        "\n",
        "def transform_fn(data_item):\n",
        "    images, _ = data_item\n",
        "    return images\n",
        "\n",
        "\n",
        "# The calibration dataset is a small, no label, representative dataset\n",
        "# (~100-500 samples) that is used to estimate the range, i.e. (min, max) of all\n",
        "# floating point activation tensors in the model, to initialize the quantization\n",
        "# parameters.\n",
        "#\n",
        "# The easiest way to define a calibration dataset is to use a training or\n",
        "# validation dataset and a transformation function to remove labels from the data\n",
        "# item and prepare model input data. The quantize method uses a small subset\n",
        "# (default: 300 samples) of the calibration dataset.\n",
        "\n",
        "calibration_dataset = nncf.Dataset(val_dataset, transform_fn)\n",
        "tf_quantized_model = nncf.quantize(tf_model, calibration_dataset)\n",
        "\n",
        "###############################################################################\n",
        "# Benchmark performance, calculate compression rate and validate accuracy\n",
        "\n",
        "ov_model = ov.convert_model(tf_model, share_weights=False)\n",
        "ov_quantized_model = ov.convert_model(tf_quantized_model, share_weights=False)\n",
        "\n",
        "fp32_ir_path = f\"{ROOT}/mobilenet_v2_fp32.xml\"\n",
        "ov.save_model(ov_model, fp32_ir_path, compress_to_fp16=False)\n",
        "print(f\"[1/7] Save FP32 model: {fp32_ir_path}\")\n",
        "fp32_model_size = get_model_size(fp32_ir_path, verbose=True)\n",
        "\n",
        "int8_ir_path = f\"{ROOT}/mobilenet_v2_int8.xml\"\n",
        "ov.save_model(ov_quantized_model, int8_ir_path, compress_to_fp16=False)\n",
        "print(f\"[2/7] Save INT8 model: {int8_ir_path}\")\n",
        "int8_model_size = get_model_size(int8_ir_path, verbose=True)\n",
        "\n",
        "print(\"[3/7] Benchmark FP32 model:\")\n",
        "fp32_fps = run_benchmark(fp32_ir_path, shape=[1, 224, 224, 3], verbose=True)\n",
        "print(\"[4/7] Benchmark INT8 model:\")\n",
        "int8_fps = run_benchmark(int8_ir_path, shape=[1, 224, 224, 3], verbose=True)\n",
        "\n",
        "print(\"[5/7] Validate OpenVINO FP32 model:\")\n",
        "fp32_top1 = validate(ov_model, val_dataset)\n",
        "print(f\"Accuracy @ top1: {fp32_top1:.3f}\")\n",
        "\n",
        "print(\"[6/7] Validate OpenVINO INT8 model:\")\n",
        "int8_top1 = validate(ov_quantized_model, val_dataset)\n",
        "print(f\"Accuracy @ top1: {int8_top1:.3f}\")\n",
        "\n",
        "print(\"[7/7] Report:\")\n",
        "print(f\"Accuracy drop: {fp32_top1 - int8_top1:.3f}\")\n",
        "print(f\"Model compression rate: {fp32_model_size / int8_model_size:.3f}\")\n",
        "# https://docs.openvino.ai/latest/openvino_docs_optimization_guide_dldt_optimization_guide.html\n",
        "print(f\"Performance speed up (throughput mode): {int8_fps / fp32_fps:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NfuyjJ0fCQk",
        "outputId": "eb1ab8d7-c3e7-4179-fa79-1cbf76deb3d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, openvino\n",
            "WARNING:nncf:NNCF provides best results with tensorflow==2.12.*, while current tensorflow version is 2.14.0. If you encounter issues, consider switching to tensorflow==2.12.*\n",
            "INFO:nncf:Creating compression algorithm: quantization\n",
            "INFO:nncf:Overflow issue fix was applied to first convolution weight quantizers.\n",
            "INFO:nncf:Collecting tensor statistics/data |█████           | 1 / 3\n",
            "INFO:nncf:Collecting tensor statistics/data |██████████      | 2 / 3\n",
            "INFO:nncf:Collecting tensor statistics/data |████████████████| 3 / 3\n",
            "INFO:nncf:BatchNorm statistics adaptation |█████           | 1 / 3\n",
            "INFO:nncf:BatchNorm statistics adaptation |██████████      | 2 / 3\n",
            "INFO:nncf:BatchNorm statistics adaptation |████████████████| 3 / 3\n",
            "[1/7] Save FP32 model: /content/mobilenet_v2_fp32.xml\n",
            "Model graph (xml):   0.071 Mb\n",
            "Model weights (bin): 16.391 Mb\n",
            "Model size:          16.463 Mb\n",
            "[2/7] Save INT8 model: /content/mobilenet_v2_int8.xml\n",
            "Model graph (xml):   0.217 Mb\n",
            "Model weights (bin): 4.224 Mb\n",
            "Model size:          4.441 Mb\n",
            "[3/7] Benchmark FP32 model:\n",
            "[ INFO ] Count:            816 iterations\n",
            "[ INFO ] Duration:         15045.34 ms\n",
            "[ INFO ] Latency:\n",
            "[ INFO ]    Median:        31.54 ms\n",
            "[ INFO ]    Average:       36.69 ms\n",
            "[ INFO ]    Min:           26.61 ms\n",
            "[ INFO ]    Max:           101.58 ms\n",
            "[ INFO ] Throughput:   54.24 FPS\n",
            "[4/7] Benchmark INT8 model:\n",
            "[ INFO ] Count:            1182 iterations\n",
            "[ INFO ] Duration:         15031.99 ms\n",
            "[ INFO ] Latency:\n",
            "[ INFO ]    Median:        21.61 ms\n",
            "[ INFO ]    Average:       25.27 ms\n",
            "[ INFO ]    Min:           13.48 ms\n",
            "[ INFO ]    Max:           73.73 ms\n",
            "[ INFO ] Throughput:   78.63 FPS\n",
            "[5/7] Validate OpenVINO FP32 model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31/31 [02:21<00:00,  4.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy @ top1: 0.006\n",
            "[6/7] Validate OpenVINO INT8 model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31/31 [01:02<00:00,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy @ top1: 0.003\n",
            "[7/7] Report:\n",
            "Accuracy drop: 0.003\n",
            "Model compression rate: 3.707\n",
            "Performance speed up (throughput mode): 1.450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference FP32 model (OpenVINO IR)\n",
        "!benchmark_app -m \"mobilenet_v2_fp32.xml\" -d $device.value -api async -t 15 -shape \"[1,224,224,3]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5PVAvCuhL8d",
        "outputId": "bef8ab22-e67a-4aea-e41c-839b80b84c2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 1/11] Parsing and validating input arguments\n",
            "[ INFO ] Parsing input parameters\n",
            "[Step 2/11] Loading OpenVINO Runtime\n",
            "[ INFO ] OpenVINO:\n",
            "[ INFO ] Build ................................. 2023.2.0-13089-cfd42bd2cb0-HEAD\n",
            "[ INFO ] \n",
            "[ INFO ] Device info:\n",
            "[ INFO ] \n",
            "[ INFO ] \n",
            "[Step 3/11] Setting device configuration\n",
            "[ ERROR ] Exception from src/inference/src/core.cpp:244:\n",
            "Exception from src/inference/src/dev/core_impl.cpp:1184:\n",
            "Exception is thrown while trying to call get_property with unsupported property: 'SUPPORTED_PROPERTIES'\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openvino/tools/benchmark/main.py\", line 165, in main\n",
            "    supported_properties = benchmark.core.get_property(device, properties.supported_properties())\n",
            "RuntimeError: Exception from src/inference/src/core.cpp:244:\n",
            "Exception from src/inference/src/dev/core_impl.cpp:1184:\n",
            "Exception is thrown while trying to call get_property with unsupported property: 'SUPPORTED_PROPERTIES'\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference INT8 model (OpenVINO IR)\n",
        "!benchmark_app -m \"mobilenet_v2_int8.xml\" -d $device.value -api async -t 15 -shape \"[32,224,224,3]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hhD-a5xhV6l",
        "outputId": "63e6963a-ae0c-4684-8fdd-c25702082863"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 1/11] Parsing and validating input arguments\n",
            "[ INFO ] Parsing input parameters\n",
            "[Step 2/11] Loading OpenVINO Runtime\n",
            "[ INFO ] OpenVINO:\n",
            "[ INFO ] Build ................................. 2023.2.0-13089-cfd42bd2cb0-HEAD\n",
            "[ INFO ] \n",
            "[ INFO ] Device info:\n",
            "[ INFO ] \n",
            "[ INFO ] \n",
            "[Step 3/11] Setting device configuration\n",
            "[ ERROR ] Exception from src/inference/src/core.cpp:244:\n",
            "Exception from src/inference/src/dev/core_impl.cpp:1184:\n",
            "Exception is thrown while trying to call get_property with unsupported property: 'SUPPORTED_PROPERTIES'\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openvino/tools/benchmark/main.py\", line 165, in main\n",
            "    supported_properties = benchmark.core.get_property(device, properties.supported_properties())\n",
            "RuntimeError: Exception from src/inference/src/core.cpp:244:\n",
            "Exception from src/inference/src/dev/core_impl.cpp:1184:\n",
            "Exception is thrown while trying to call get_property with unsupported property: 'SUPPORTED_PROPERTIES'\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}